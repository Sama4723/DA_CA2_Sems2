{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e46b233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670a2056",
   "metadata": {},
   "source": [
    "### Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51e93ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SparkSession library \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('data_processing').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3474923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Load the libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import udf, StringType\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler, StringIndexer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "559a12b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, substring, from_unixtime, unix_timestamp, lit, to_timestamp, to_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7391dd",
   "metadata": {},
   "source": [
    "### Loading the original ProjectTweets.csv dataset before doing the analysis on hive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "172c88b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: integer (nullable = true)\n",
      " |-- ids: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- falg: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Schema = StructType([\n",
    "    StructField(\"index\", IntegerType(), True), \n",
    "    StructField(\"ids\", IntegerType(), True), \n",
    "    StructField(\"date\", StringType(), True), \n",
    "    StructField(\"falg\", StringType(), True), \n",
    "    StructField(\"user\", StringType(), True),\n",
    "     StructField(\"text\", StringType(), True)])\n",
    "    \n",
    "\n",
    "data= spark.read.load('hdfs://localhost:9000/user1/ProjectTweets.csv', format=\"csv\", header=\"true\", sep=',', schema=Schema)\n",
    "data.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d9dcfa",
   "metadata": {},
   "source": [
    "###### Checking for null values.  We see  column ids has 432913 missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b342c986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-05 20:23:04,364 WARN csv.CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 0, 1467810369, Mon Apr 06 22:19:45 PDT 2009, NO_QUERY, _TheSpecialOne_, @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      " Schema: index, ids, date, falg, user, text\n",
      "Expected: index but found: 0\n",
      "CSV file: hdfs://localhost:9000/user1/ProjectTweets.csv\n",
      "[Stage 8:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+----+----+----+\n",
      "|index|   ids|date|falg|user|text|\n",
      "+-----+------+----+----+----+----+\n",
      "|    0|432913|   0|   0|  23| 260|\n",
      "+-----+------+----+----+----+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "data2 = data.select([count(when(col(c).contains('None') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | \\\n",
    "                            col(c).isNull() | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in data.columns])\n",
    "data2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d12547e",
   "metadata": {},
   "source": [
    "#### loading the  sentiments dataset and create schema for the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca8841f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ids: integer (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- score: byte (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customSchema = StructType([\n",
    "    #StructField(\"index\", IntegerType(), True), \n",
    "    StructField(\"ids\", IntegerType(), True), \n",
    "    StructField(\"time\", StringType(), True), \n",
    "    StructField(\"score\", ByteType(), True), \n",
    "    StructField(\"sentiment\", StringType(), True)])\n",
    "    \n",
    "\n",
    "tweets_sentiment= spark.read.load('hdfs://localhost:9000//user1/tweet_sent_2.csv', format=\"csv\", header=\"false\", sep=',', schema=customSchema)\n",
    "tweets_sentiment.printSchema()\n",
    "\n",
    "#data = spark.read.csv('/user1/ProjectTweets.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08d22b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----+---------+\n",
      "| ids|                time|score|sentiment|\n",
      "+----+--------------------+-----+---------+\n",
      "|null|Fri Jun 19 00:00:...|    1| positive|\n",
      "|null|Fri Jun 19 00:00:...|   -4| negative|\n",
      "|null|Fri Jun 19 00:00:...|   -1| negative|\n",
      "|null|Fri Jun 19 00:00:...|    2| positive|\n",
      "|null|Fri Jun 19 00:00:...|   -3| negative|\n",
      "|null|Fri Jun 19 00:00:...|  -15| negative|\n",
      "|null|Fri Jun 19 00:00:...|   -3| negative|\n",
      "|null|Fri Jun 19 00:00:...|   -3| negative|\n",
      "|null|Fri Jun 19 00:00:...|    2| positive|\n",
      "|null|Fri Jun 19 00:00:...|   -4| negative|\n",
      "|null|Fri Jun 19 00:00:...|   -2| negative|\n",
      "|null|Fri Jun 19 00:00:...|   -1| negative|\n",
      "|null|Fri Jun 19 00:00:...|   -5| negative|\n",
      "|null|Fri Jun 19 00:00:...|    1| positive|\n",
      "|null|Fri Jun 19 00:00:...|    4| positive|\n",
      "|null|Fri Jun 19 00:00:...|    1| positive|\n",
      "|null|Fri Jun 19 00:00:...|   -3| negative|\n",
      "|null|Fri Jun 19 00:01:...|    2| positive|\n",
      "|null|Fri Jun 19 00:01:...|    1| positive|\n",
      "|null|Fri Jun 19 00:01:...|   11| positive|\n",
      "+----+--------------------+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_sentiment.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "944aa7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tweets_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ba98d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ids: integer (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- score: byte (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the structure of schema\n",
    "tweets_sentiment.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d79340c",
   "metadata": {},
   "source": [
    "#### The time column need to be changed to date time type column and the format of the date and time should be written in certain way to be understood by spark as a timestamp type. To do this, many functions applied first, apply the subsring function to take only the time then add the year as a string using the function lit.  The function concat to add the tow strings together. Then, the function unix_timestamp applied to put the the date and time in certain format and convert them to unix time stamp. Finally, form unix time stamp applying the function from_unixtime to put the date-time back into the required format with the correct type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59cf7950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|           datetime|\n",
      "+-------------------+\n",
      "|2009-06-19 00:00:00|\n",
      "|2009-06-19 00:00:01|\n",
      "|2009-06-19 00:00:04|\n",
      "|2009-06-19 00:00:13|\n",
      "|2009-06-19 00:00:17|\n",
      "|2009-06-19 00:00:20|\n",
      "|2009-06-19 00:00:21|\n",
      "|2009-06-19 00:00:34|\n",
      "|2009-06-19 00:00:35|\n",
      "|2009-06-19 00:00:37|\n",
      "|2009-06-19 00:00:40|\n",
      "|2009-06-19 00:00:43|\n",
      "|2009-06-19 00:00:45|\n",
      "|2009-06-19 00:00:50|\n",
      "|2009-06-19 00:00:53|\n",
      "|2009-06-19 00:00:54|\n",
      "|2009-06-19 00:00:56|\n",
      "|2009-06-19 00:01:02|\n",
      "|2009-06-19 00:01:03|\n",
      "|2009-06-19 00:01:11|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_sentiment.select( from_unixtime(unix_timestamp( concat( substring('time', 5, 6), lit(\" 2009 \"), substring('time',12,8) ) , \"MMM dd yyyy HH:mm:ss\") ).alias(\"datetime\") ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2df1ae8",
   "metadata": {},
   "source": [
    "###### save all the new changes to a new dataset named tweets_sent_df contains the ids, datetime, score and sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34eced08",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_sent_df = tweets_sentiment.select(\n",
    "    'ids',\n",
    "    from_unixtime(unix_timestamp( concat( substring('time', 5, 6), lit(\" 2009 \"), substring('time',12,8) ) , \"MMM dd yyyy HH:mm:ss\") ).alias(\"datetime\"),\n",
    "    'score',\n",
    "    'sentiment'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de01592d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+-----+---------+\n",
      "| ids|           datetime|score|sentiment|\n",
      "+----+-------------------+-----+---------+\n",
      "|null|2009-06-19 00:00:00|    1| positive|\n",
      "|null|2009-06-19 00:00:01|   -4| negative|\n",
      "|null|2009-06-19 00:00:04|   -1| negative|\n",
      "|null|2009-06-19 00:00:13|    2| positive|\n",
      "|null|2009-06-19 00:00:17|   -3| negative|\n",
      "|null|2009-06-19 00:00:20|  -15| negative|\n",
      "|null|2009-06-19 00:00:21|   -3| negative|\n",
      "|null|2009-06-19 00:00:34|   -3| negative|\n",
      "|null|2009-06-19 00:00:35|    2| positive|\n",
      "|null|2009-06-19 00:00:37|   -4| negative|\n",
      "|null|2009-06-19 00:00:40|   -2| negative|\n",
      "|null|2009-06-19 00:00:43|   -1| negative|\n",
      "|null|2009-06-19 00:00:45|   -5| negative|\n",
      "|null|2009-06-19 00:00:50|    1| positive|\n",
      "|null|2009-06-19 00:00:53|    4| positive|\n",
      "|null|2009-06-19 00:00:54|    1| positive|\n",
      "|null|2009-06-19 00:00:56|   -3| negative|\n",
      "|null|2009-06-19 00:01:02|    2| positive|\n",
      "|null|2009-06-19 00:01:03|    1| positive|\n",
      "|null|2009-06-19 00:01:11|   11| positive|\n",
      "+----+-------------------+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_sent_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c579e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_sent_new_df = tweets_sent_df.select('ids', to_timestamp(tweets_sent_df.datetime, 'yyyy-MM-dd HH:mm:ss').alias('datetime_new'), 'score', 'sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fdc5a18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+-----+---------+\n",
      "| ids|       datetime_new|score|sentiment|\n",
      "+----+-------------------+-----+---------+\n",
      "|null|2009-06-19 00:00:00|    1| positive|\n",
      "|null|2009-06-19 00:00:01|   -4| negative|\n",
      "|null|2009-06-19 00:00:04|   -1| negative|\n",
      "|null|2009-06-19 00:00:13|    2| positive|\n",
      "|null|2009-06-19 00:00:17|   -3| negative|\n",
      "|null|2009-06-19 00:00:20|  -15| negative|\n",
      "|null|2009-06-19 00:00:21|   -3| negative|\n",
      "|null|2009-06-19 00:00:34|   -3| negative|\n",
      "|null|2009-06-19 00:00:35|    2| positive|\n",
      "|null|2009-06-19 00:00:37|   -4| negative|\n",
      "|null|2009-06-19 00:00:40|   -2| negative|\n",
      "|null|2009-06-19 00:00:43|   -1| negative|\n",
      "|null|2009-06-19 00:00:45|   -5| negative|\n",
      "|null|2009-06-19 00:00:50|    1| positive|\n",
      "|null|2009-06-19 00:00:53|    4| positive|\n",
      "|null|2009-06-19 00:00:54|    1| positive|\n",
      "|null|2009-06-19 00:00:56|   -3| negative|\n",
      "|null|2009-06-19 00:01:02|    2| positive|\n",
      "|null|2009-06-19 00:01:03|    1| positive|\n",
      "|null|2009-06-19 00:01:11|   11| positive|\n",
      "+----+-------------------+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_sent_new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8deea354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ids: integer (nullable = true)\n",
      " |-- datetime_new: timestamp (nullable = true)\n",
      " |-- score: byte (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_sent_new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84ef40e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tweets_sent_new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f966d60b",
   "metadata": {},
   "source": [
    "#### save the processed dataset to hadoop as a csv file to be able to apply the deep learning part on it. Here we see error because I ran it many times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f007f39e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path hdfs://localhost:9000/user1/tweets_sent_new_df.csv already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9589/1755073753.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweets_sent_new_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hdfs://localhost:9000//user1/tweets_sent_new_df.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m    953\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n\u001b[0;32m--> 955\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0morc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitionBy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path hdfs://localhost:9000/user1/tweets_sent_new_df.csv already exists."
     ]
    }
   ],
   "source": [
    "tweets_sent_new_df.write.option(\"header\",True).csv(\"hdfs://localhost:9000//user1/tweets_sent_new_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51ea1981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Casting to unit-less dtype 'datetime64' is not supported. Pass e.g. 'datetime64[ns]' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9589/999148835.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweets_sent_new_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tweets_sent_new_df.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m                 \u001b[0mseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;31m# `insert` API makes copy of data, we only do it for Series of duplicate column names.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6532\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6533\u001b[0m             \u001b[0;31m# else, only a single dtype is given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6534\u001b[0;31m             \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6535\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_from_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6536\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         return self.apply(\n\u001b[0m\u001b[1;32m    415\u001b[0m             \u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors, using_cow)\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_coerce_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;31m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;31m# i.e. ExtensionArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/arrays/datetimes.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy)\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mis_unitless\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         ):\n\u001b[0;32m--> 720\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    721\u001b[0m                 \u001b[0;34m\"Casting to unit-less dtype 'datetime64' is not supported. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0;34m\"Pass e.g. 'datetime64[ns]' instead.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Casting to unit-less dtype 'datetime64' is not supported. Pass e.g. 'datetime64[ns]' instead."
     ]
    }
   ],
   "source": [
    "tweets_sent_new_df.toPandas().to_csv('tweets_sent_new_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9509292",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
